{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess training data\n",
    "- strip\n",
    "- for Vietnamese sentences:\n",
    "    - add prefix `\"translate Vietnamese to English: \"` at the beginning\n",
    "    - add `\" </s>\" at the end\n",
    "- save everything as jsonline format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import jsonlines\n",
    "\n",
    "# subsets = [\"train\", \"dev\", \"test\"]\n",
    "# langs = [\"en\", \"vi\"]\n",
    "\n",
    "# for subset in subsets:\n",
    "#     temp = {}\n",
    "#     data = []\n",
    "#     for lang in langs:\n",
    "#         path = os.path.join(\"data/PhoMT/tokenization\",subset,f\"{subset}.{lang}\")\n",
    "#         with open(path, \"r\", encoding='utf-8') as f:    \n",
    "#             contents = f.readlines()\n",
    "#         print(f\"{path}:\", len(contents), \"lines\")\n",
    "#         for i in range(len(contents)):\n",
    "#             line = contents[i].strip()\n",
    "#             if line[-1:] == \"\\n\":\n",
    "#                 line = line[:-1]\n",
    "#             if lang == \"vi\":\n",
    "#                 contents[i] = \"translate Vietnamese to English: \" + line + \" </s>\"\n",
    "#             else:\n",
    "#                 contents[i] = line\n",
    "#         temp[lang] = contents\n",
    "#     for en, vi in zip(temp[\"en\"], temp[\"vi\"]):\n",
    "#         data.append({\n",
    "#             \"en\": en,\n",
    "#             \"vi\": vi\n",
    "#         })\n",
    "#     with jsonlines.open(f'data/PhoMT/tokenization/{subset}.jsonl', mode='w') as writer:        \n",
    "#         writer.write_all(data)\n",
    "# del temp\n",
    "# del data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init wandb for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuyhuy\u001b[0m (\u001b[33mfantastic-four\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cotai/projects/vn-translate/src/notebooks/wandb/run-20220916_083824-s4adlr17</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fantastic-four/vietai-machine-translation/runs/s4adlr17\" target=\"_blank\">vit5_chkp-30000</a></strong> to <a href=\"https://wandb.ai/fantastic-four/vietai-machine-translation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/fantastic-four/vietai-machine-translation/runs/s4adlr17?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0903b8abe0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"vietai-machine-translation\", entity=\"fantastic-four\", name=\"vit5_chkp-30000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset from jsonlines files\n",
    "\n",
    "Use streaming to reduce RAM usage, need to convert the dataset to `torch` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-961e8ec10ee50d7d\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\", \n",
    "    data_files={\n",
    "        \"train\":\"data/PhoMT/tokenization/train.jsonl\",\n",
    "        \"dev\":\"data/PhoMT/tokenization/dev.jsonl\",\n",
    "        \"test\":\"data/PhoMT/tokenization/test.jsonl\"\n",
    "    },\n",
    "    streaming=True\n",
    ")\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base\").to(\"cuda:1\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"checkpoint-40000\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"checkpoint-40000\").to(\"cuda:1\")\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model.use_cache = False\n",
    "max_input_length = 128\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"vi\"] \n",
    "    targets =examples[\"en\"] \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"en\", \"vi\"]\n",
    ")\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check RAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 2607.81 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compute sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_train = 64\n",
    "num_epochs = 2\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"vit5-phoMT\",\n",
    "    evaluation_strategy = 'steps',\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps = 500,                   \n",
    "    eval_steps = 5000, \n",
    "    save_steps=1000,\n",
    "    learning_rate=0.001,\n",
    "    per_device_eval_batch_size=128,\n",
    "    save_total_limit=3,\n",
    "    max_steps= 40000,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    half_precision_backend = \"auto\",\n",
    "    # report_to=\"wandb\",  # enable logging to W&B\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=8\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 2977999 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "optim = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=True, \n",
    "    warmup_init=True, \n",
    "    lr=None\n",
    ")\n",
    "lr_scheduler = AdafactorSchedule(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optim, lr_scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\"checkpoint-40000\") # continue training\n",
    "# trainer.train() # from stratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 1 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local elders and the neighborhood supervisors are helping and providing physical and mentality to the brothers that are affected by this disaster.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Các trưởng lão địa phương và giám thị xung quanh đang giúp đỡ và cung cấp về vật chất và tinh thần cho các anh chị bị ảnh hưởng trong thảm hoạ này .\"\n",
    "text =  \"translate Vietnamese to English: \" + sentence + \" </s>\"\n",
    "encoding = tokenizer(text, padding=True, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda:1\"), encoding[\"attention_mask\"].to(\"cuda:1\")\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=max_target_length,\n",
    "    # early_stopping=True,\n",
    "    # do_sample=True,\n",
    "    # num_beams=5,\n",
    "    # num_return_sequences=1,\n",
    "    # no_repeat_ngram_size=1,\n",
    "    # remove_invalid_values=True,\n",
    ")\n",
    "for output in outputs:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"Local elders and the circuit overseer are offering practical and spiritual support to those affected by this disaster .\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vietai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eb15e64409fcc6543c0c9fa63971baa43084f006bc3c4bb3699f01076ad9f00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
