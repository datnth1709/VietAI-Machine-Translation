{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import jsonlines\n",
    "\n",
    "# subsets = [\"train\", \"dev\", \"test\"]\n",
    "# langs = [\"en\", \"vi\"]\n",
    "\n",
    "# for subset in subsets:\n",
    "#     temp = {}\n",
    "#     data = []\n",
    "#     for lang in langs:\n",
    "#         path = os.path.join(\"../data/PhoMT/tokenization\",subset,f\"{subset}.{lang}\")\n",
    "#         with open(path, \"r\", encoding='utf-8') as f:    \n",
    "#             contents = f.readlines()\n",
    "#         print(f\"{path}:\", len(contents), \"lines\")\n",
    "#         for i in range(len(contents)):\n",
    "#             line = contents[i].strip()\n",
    "#             if line[-1:] == \"\\n\":\n",
    "#                 line = line[:-1]\n",
    "#             if lang == \"vi\":\n",
    "#                 contents[i] = \"translate Vietnamese to English: \" + line + \" </s>\"\n",
    "#             else:\n",
    "#                 contents[i] = line\n",
    "#         temp[lang] = contents\n",
    "#     for en, vi in zip(temp[\"en\"], temp[\"vi\"]):\n",
    "#         data.append({\n",
    "#             \"en\": en,\n",
    "#             \"vi\": vi\n",
    "#         })\n",
    "#     with jsonlines.open(f'../data/PhoMT/tokenization/{subset}.jsonl', mode='w') as writer:        \n",
    "#         writer.write_all(data)\n",
    "# del temp\n",
    "# del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuyhuy\u001b[0m (\u001b[33mfantastic-four\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cotai/projects/vn-translate/src/notebooks/wandb/run-20220916_083824-s4adlr17</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fantastic-four/vietai-machine-translation/runs/s4adlr17\" target=\"_blank\">vit5_chkp-30000</a></strong> to <a href=\"https://wandb.ai/fantastic-four/vietai-machine-translation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/fantastic-four/vietai-machine-translation/runs/s4adlr17?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0903b8abe0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"vietai-machine-translation\", entity=\"fantastic-four\", name=\"vit5_chkp-30000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-961e8ec10ee50d7d\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\", \n",
    "    data_files={\n",
    "        \"train\":\"../data/PhoMT/tokenization/train.jsonl\",\n",
    "        \"dev\":\"../data/PhoMT/tokenization/dev.jsonl\",\n",
    "        \"test\":\"../data/PhoMT/tokenization/test.jsonl\"\n",
    "    },\n",
    "    streaming=True\n",
    ")\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base\").to(\"cuda:1\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vit5-base-3/checkpoint-30000\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"vit5-base-3/checkpoint-30000\").to(\"cuda:1\")\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model.use_cache = False\n",
    "max_input_length = 128\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"vi\"] \n",
    "    targets =examples[\"en\"] \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"en\", \"vi\"]\n",
    ")\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 2607.81 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_train = 64\n",
    "num_epochs = 2\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"vit5-base-4\",\n",
    "    evaluation_strategy = 'steps',\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps = 500,                   \n",
    "    eval_steps = 5000, \n",
    "    save_steps=1000,\n",
    "    learning_rate=0.001,\n",
    "    per_device_eval_batch_size=128,\n",
    "    save_total_limit=3,\n",
    "    max_steps= 40000,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    half_precision_backend = \"auto\",\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    # run_name=\"vit5-1\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=8\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 2977999 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "optim = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=True, \n",
    "    warmup_init=True, \n",
    "    lr=None\n",
    ")\n",
    "lr_scheduler = AdafactorSchedule(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optim, lr_scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 256\n",
      "/home/cotai/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[    0,  5559,  1827, ...,  6853,  2695,     5],\n",
       "       [    0,  3222,    40, ...,  2165, 35807,  4768],\n",
       "       [    0,  1746, 26159, ...,  1402, 12661,  6855],\n",
       "       ...,\n",
       "       [    0,  1263, 28186, ...,   940,  2273, 14961],\n",
       "       [    0,  3600, 22214, ..., 32926,     5, 11196],\n",
       "       [    0, 14284, 24770, ...,  2282,  2174,  2273]]), label_ids=array([[29332,  5559,  1827, ...,  -100,  -100,  -100],\n",
       "       [ 5111,  4398,  3826, ...,  -100,  -100,  -100],\n",
       "       [12131,    35,  7487, ...,  -100,  -100,  -100],\n",
       "       ...,\n",
       "       [ 1263, 28186,     5, ...,  -100,  -100,  -100],\n",
       "       [25162,  8652, 35793, ...,  -100,  -100,  -100],\n",
       "       [   26,  4019,  4829, ...,  -100,  -100,  -100]]), metrics={'test_loss': 1.4580744504928589, 'test_bleu': 14.853603979613457, 'test_runtime': 118.9247, 'test_samples_per_second': 161.035, 'test_steps_per_second': 0.631})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from vit5-base-3/checkpoint-30000.\n",
      "***** Running training *****\n",
      "  Num examples = 10240000\n",
      "  Num Epochs = 9223372036854775807\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 40000\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 30000\n",
      "  Will skip the first 0 epochs then the first 240000 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0075740814208984375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 240000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e97ee07a92e44f096959882cd8bd9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/home/cotai/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30038' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30038/40000 01:25 < 6:33:55, 0.42 it/s, Epoch 1.17/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cotai/projects/vn-translate/src/notebooks/vit.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B14.161.19.83/home/cotai/projects/vn-translate/src/notebooks/vit.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m\"\u001b[39;49m\u001b[39mvit5-base-3/checkpoint-30000\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/transformers/trainer.py:1499\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1496\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1497\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1498\u001b[0m )\n\u001b[0;32m-> 1499\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1500\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1501\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1502\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1503\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1504\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/transformers/trainer.py:1741\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1739\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1740\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1741\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1743\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1744\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1745\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1746\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1747\u001b[0m ):\n\u001b[1;32m   1748\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/transformers/trainer.py:2476\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2475\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2476\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2479\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/transformers/trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2507\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2508\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2509\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2510\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplicate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(inputs)])\n\u001b[1;32m    168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:172\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreplicate\u001b[39m(\u001b[39mself\u001b[39m, module, device_ids):\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m replicate(module, device_ids, \u001b[39mnot\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mis_grad_enabled())\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/replicate.py:91\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m     89\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(network\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m     90\u001b[0m param_indices \u001b[39m=\u001b[39m {param: idx \u001b[39mfor\u001b[39;00m idx, param \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(params)}\n\u001b[0;32m---> 91\u001b[0m param_copies \u001b[39m=\u001b[39m _broadcast_coalesced_reshape(params, devices, detach)\n\u001b[1;32m     93\u001b[0m buffers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(network\u001b[39m.\u001b[39mbuffers())\n\u001b[1;32m     94\u001b[0m buffers_rg \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/replicate.py:71\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tensors) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m         tensor_copies \u001b[39m=\u001b[39m Broadcast\u001b[39m.\u001b[39;49mapply(devices, \u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m     72\u001b[0m         \u001b[39mreturn\u001b[39;00m [tensor_copies[i:i \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(tensors)]\n\u001b[1;32m     73\u001b[0m                 \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tensor_copies), \u001b[39mlen\u001b[39m(tensors))]\n\u001b[1;32m     74\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:23\u001b[0m, in \u001b[0;36mBroadcast.forward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m ctx\u001b[39m.\u001b[39mnum_inputs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(inputs)\n\u001b[1;32m     22\u001b[0m ctx\u001b[39m.\u001b[39minput_device \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_device()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[39m=\u001b[39m comm\u001b[39m.\u001b[39;49mbroadcast_coalesced(inputs, ctx\u001b[39m.\u001b[39;49mtarget_gpus)\n\u001b[1;32m     24\u001b[0m non_differentiables \u001b[39m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m idx, input_requires_grad \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ctx\u001b[39m.\u001b[39mneeds_input_grad[\u001b[39m1\u001b[39m:]):\n",
      "File \u001b[0;32m~/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/comm.py:58\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m devices \u001b[39m=\u001b[39m [_get_device_index(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m devices]\n\u001b[1;32m     57\u001b[0m tensors \u001b[39m=\u001b[39m [_handle_complex(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tensors]\n\u001b[0;32m---> 58\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_broadcast_coalesced(tensors, devices, buffer_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\"vit5-base-3/checkpoint-30000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 256\n",
      "/home/cotai/anaconda3/envs/vietai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.6363742351531982,\n",
       " 'eval_bleu': 15.480611927582189,\n",
       " 'eval_runtime': 183.5569,\n",
       " 'eval_samples_per_second': 101.979,\n",
       " 'eval_steps_per_second': 0.403}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local elders and the neighborhood supervisors are helping and providing physical and mentality to the brothers that are affected by this disaster.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Các trưởng lão địa phương và giám thị xung quanh đang giúp đỡ và cung cấp về vật chất và tinh thần cho các anh chị bị ảnh hưởng trong thảm hoạ này .\"\n",
    "text =  \"translate Vietnamese to English: \" + sentence + \" </s>\"\n",
    "encoding = tokenizer(text, padding=True, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda:1\"), encoding[\"attention_mask\"].to(\"cuda:1\")\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=max_target_length,\n",
    "    # early_stopping=True,\n",
    "    # do_sample=True,\n",
    "    # num_beams=5,\n",
    "    # num_return_sequences=1,\n",
    "    # no_repeat_ngram_size=1,\n",
    "    # remove_invalid_values=True,\n",
    ")\n",
    "for output in outputs:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"Local elders and the circuit overseer are offering practical and spiritual support to those affected by this disaster .\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vietai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eb15e64409fcc6543c0c9fa63971baa43084f006bc3c4bb3699f01076ad9f00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
